# llm-inference-solutions
A collection of all available inference solutions for the LLMs

| Name  | Org  | Description | 
| ------------- |:-------------:| :-------------:|
|   [vllm](https://github.com/vllm-project/vllm)    |  UC Berkeley    |  A high-throughput and memory-efficient inference and serving engine for LLMs
| [Text-Generation-Inference](https://github.com/huggingface/text-generation-inference)      | HugginfaceðŸ¤—     |Large Language Model Text Generation Inference
| [llm-engine](https://github.com/scaleapi/llm-engine)      | ScaleAI     |Scale LLM Engine public repository
| [DeepSpeed](https://github.com/microsoft/DeepSpeed) | Microsoft | DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective
| [OpenLLM](https://github.com/bentoml/OpenLLM) | BentoML | Operating LLMs in production

